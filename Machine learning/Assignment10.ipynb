{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the training, the parameters (weights and bias) of the neural network instance should be printed and the error curve over the epochs should be plotted in logarithmic scale. Furthermore, the target data and the data as outputted by the final neural network is shown in a scatter plot. Since the input data is generated randomly, you may expect different plots for every execution of the notebook. The whole function is called twice, one time for each dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(threshold=10) # printing setup\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This formula shall be imitated by the neural network\n",
    "def formula_1(x):\n",
    "    return x + 0.75\n",
    "\n",
    "# This formula shall be imitated by the neural network, too\n",
    "def formula_2(x):\n",
    "    return(np.sin(x) + 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.83618953]\n",
      " [ 1.43673868]\n",
      " [ 0.93389609]\n",
      " ...\n",
      " [-0.36146105]\n",
      " [ 0.49385768]\n",
      " [-0.15143711]]\n"
     ]
    }
   ],
   "source": [
    "# Input data\n",
    "X = np.random.uniform(low=-0.5, high=1.55, size=100)# make 100 uniformly distributed samples\n",
    "X = X.reshape(-1, 1) # -1 indicates \"as many rows as required\"\n",
    "print(X) # input data as column vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.58618953],\n",
       "       [2.18673868],\n",
       "       [1.68389609],\n",
       "       ...,\n",
       "       [0.38853895],\n",
       "       [1.24385768],\n",
       "       [0.59856289]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Target values with formula_1\n",
    "y_1 = np.array([formula_1(x) for x in X.flatten()]) # one output per sample, |X|-many samples\n",
    "y_1 = y_1.reshape(-1, 1)\n",
    "y_1 # target data as column vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.24209437],\n",
       "       [1.49102772],\n",
       "       [1.30394307],\n",
       "       ...,\n",
       "       [0.14635875],\n",
       "       [0.97402613],\n",
       "       [0.34914105]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Target values with formula_2\n",
    "y_2 = np.array([formula_2(x) for x in X.flatten()]) # one output per sample, |X|-many samples\n",
    "y_2 = y_2.reshape(-1, 1)\n",
    "y_2 # target data as column vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code below contains the parts to be edited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-7-a477fbfda57f>, line 45)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-7-a477fbfda57f>\"\u001b[0;36m, line \u001b[0;32m45\u001b[0m\n\u001b[0;31m    def execute_nn(X, y):\u001b[0m\n\u001b[0m                         ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "# Simple neural-network based regressor\n",
    "class NeuralNetwork:\n",
    "    \n",
    "    # Function called at object initialization\n",
    "    def __init__(self):\n",
    "        \n",
    "        # These are members of the class. You can access them in every method by \"self.var_name\" and from outside the class with \"instance_name.var_name\"\n",
    "        \n",
    "        # Sample to compute pass with\n",
    "        self.X          = 0.0 # set me!\n",
    "        self.y          = 0.0 # set me!\n",
    "        \n",
    "        # Parameters to be learned\n",
    "        self.weight_1   = 1.0 #np.random.uniform(low=-0.01, high=0.80, size=100) # teach me!\n",
    "        self.weight_2   = 1.0 #np.random.uniform(low=-0.02, high=0.90, size=100) # teach me!\n",
    "        self.bias       = 1.0 # teach me!\n",
    "        \n",
    "        # State information\n",
    "        self.hidden     = 0.0 # use me!\n",
    "        self.output     = 0.0 # use me!\n",
    "        self.error      = 0.0 # use me!\n",
    "        \n",
    "    # Set sample to be used in feed-forward and back-propagation pass\n",
    "    def set_sample(self, X, y):\n",
    "        self.X = float(X)\n",
    "        self.y = float(y)\n",
    "        \n",
    "    # (a) Feed-forward pass\n",
    "    def feed_forward(self):\n",
    "        self.hidden= self.X* self.weight_1 + self.bias\n",
    "        self.hidden= 1/(1+np.exp(-self.hidden))\n",
    "        self.output= self.hidden* self.weight_2\n",
    "        return 0.5* np.square(self.output- self.y)\n",
    "        \n",
    "        # TODO: Feed-forward pass and error estimation, store results in state information members\n",
    "        # Consider the loss function: 0.5 ‚àó (ùë°ùëéùëüùëîùëíùë° ‚àí ùëúùë¢ùë°ùëùùë¢ùë°)2\n",
    "\n",
    "      \n",
    "    # (b) Back-propagation pass\n",
    "    def back_prop(self):\n",
    "        \n",
    "        # TODO: Derivations of parameters\n",
    "        # TODO: Update the parameters with learning rate of 0.01\n",
    "        \n",
    "def execute_nn(X, y):\n",
    "    \n",
    "    # Instantiate neural network\n",
    "    nn = NeuralNetwork()\n",
    "    \n",
    "    # Collect mean error of each epoch to plot it later\n",
    "    epoch_error = []\n",
    "\n",
    "    # Perform multiple epochs, aka inputting the dataset multiple times\n",
    "    for epoch in range(0,100):\n",
    "        error=[]\n",
    "        for i in range(0, len(X)):\n",
    "            nn.set_sample(X[i], y[i])\n",
    "            error.append(nn.feed_forward())\n",
    "            nn.back_prop()\n",
    "        epoch_error.append(np.mean(error))\n",
    "\n",
    "        \n",
    "        # Example use of neural network class\n",
    "        # nn = NeuralNetwork() # instantiates neural network\n",
    "        # nn.set_sample(2,5) # sets sample with 2 as input and 5 as target\n",
    "        # nn.feed_forward() # perform feed-forward to calculate output\n",
    "        # nn.back_prop() # use difference between target and actual output to update parameters\n",
    "        #Remember final error of each epoch in \"epoch_error\"\n",
    "        print(\"TODO (c)\") # remove this line, just here to produce no compile error in initial setup\n",
    "        \n",
    "    # Print final parameters of trained neural network\n",
    "    print(\"Weight_1:\"+ str(nn.weight_1))\n",
    "    print(\"Weight_2:\" + str(nn.weight_2))\n",
    "    print(\"Bias:\" + str(nn.bias))\n",
    "    \n",
    "    # Plot epoch errors with logarithmic transformation\n",
    "    plt.plot(list(range(len(epoch_error))), np.log(epoch_error))\n",
    "    ax = plt.gca()\n",
    "    ax.set_xlabel('# Epoch')\n",
    "    ax.set_ylabel('Error')\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot datapoints as originally transformed and as transformed by neural network\n",
    "    computed = []\n",
    "    for i in range(0, X.shape[0]):\n",
    "        nn.set_sample(X[i], y[i])\n",
    "        nn.feed_forward()\n",
    "        computed.append(nn.output)\n",
    "    plt.scatter(X.transpose().flatten(), y.transpose().flatten(), c='blue', s=16)\n",
    "    plt.scatter(X.transpose().flatten(), computed, c='red', s=16)\n",
    "    plt.show()\n",
    "\n",
    "print(\"Imitation of formula 'x+0.75'\")\n",
    "execute_nn(X,y_1)\n",
    "print()\n",
    "print(\"Imitation of formula 'sin(x)+0.5'\")\n",
    "execute_nn(X,y_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
