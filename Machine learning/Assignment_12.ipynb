{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Implement a class that contains the following methods:\n",
    "\n",
    "A constructor that takes two parameters: a list of the predicted classes, and a list of the ground truth.\n",
    "\"getPrecision\": a function that returns the precision of the given predicted and ground truth lists. \n",
    "\"getRecall\": a function that returns the recall of the given predicted and ground truth lists. \n",
    "\"getPrecision\": a function that returns the precision of the given predicted and ground truth lists. \n",
    "\"getAccuracy\": a function that returns the accuracy of the given predicted and ground truth lists. \n",
    "\"getF1Score\": a function that returns the F1-score of the given predicted and ground truth lists.\n",
    "\"printEvaluation\": a function that visualizes all the evaluation metrics calculated in a form of a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Assignment12:\n",
    "    def __init__(self, truth, predicted ):    #constructor\n",
    "        self.truth= truth\n",
    "        self.predicted= predicted\n",
    "        self.n= len(self.truth)\n",
    "        \n",
    "    def getTruePositive(self):\n",
    "        tp0=0\n",
    "        tp1=0\n",
    "        \n",
    "        for i in range(self.n):\n",
    "            if(self.truth[i]==0):       #for 0 class\n",
    "                if(self.truth[i] == self.predicted[i]):\n",
    "                    tp0+=1\n",
    "        \n",
    "        for i in range(self.n):\n",
    "            if(self.truth[i]==1):       #for 0 class\n",
    "                if(self.truth[i] == self.predicted[i]):\n",
    "                    tp1+=1\n",
    "        return (tp0,tp1)\n",
    "    \n",
    "    def getPrecision(self):                  #precision\n",
    "        tp0, tp1= self.getTruePositive()\n",
    "        retrieved0=0\n",
    "        for i in range(self.n):\n",
    "            if(self.predicted[i]==0):\n",
    "                retrieved0+=1\n",
    "        precision0= tp0/retrieved0\n",
    "        \n",
    "        retrieved1=0\n",
    "        for i in range(self.n):\n",
    "            if(self.predicted[i]==1):\n",
    "                retrieved1+=1\n",
    "        precision1= tp1/retrieved1\n",
    "        return (precision0, precision1)\n",
    "    \n",
    "    def getRecall(self):                         #recall\n",
    "        tp0, tp1= self.getTruePositive()\n",
    "        relevant0=0\n",
    "        for i in range(self.n):\n",
    "            if(self.truth[i]==0):\n",
    "                relevant0+=1\n",
    "        recall0= tp0/relevant0\n",
    "        \n",
    "        relevant1=0\n",
    "        for i in range(self.n):\n",
    "            if(self.truth[i]==1):\n",
    "                relevant1+=1\n",
    "        recall1= tp1/relevant1\n",
    "        return (recall0, recall1)\n",
    "        \n",
    "    def getAccuracy(self):                 #Accuracy\n",
    "        tp0,tp1= self.getTruePositive()\n",
    "        accuracy0= tp0/len(self.truth)\n",
    "        accuracy1= tp1/len(self.truth)\n",
    "        return (accuracy0,accuracy1)\n",
    "\n",
    "    def getF1Score(self):               #F1 Score\n",
    "        pre0, pre1= self.getPrecision()\n",
    "        rec0, rec1= self.getRecall()\n",
    "        \n",
    "        f10= (2 * pre0 * rec0)/ (pre0 + rec0)\n",
    "        f11= (2 * pre1 * rec1)/ (pre1 + rec1)\n",
    "        \n",
    "        return (f10, f11)\n",
    "    \n",
    "    def printEvaluation(self):            #Evaluation\n",
    "        pre0, pre1= self.getPrecision()\n",
    "        rec0, rec1= self.getRecall()\n",
    "        f10, f11= self.getF1Score()\n",
    "        acc0, acc1= self.getAccuracy()\n",
    "        \n",
    "        dict1={'Precision':[pre0, pre1], 'Recall':[rec0, rec1], 'F1Score':[f10, f11], 'Accuracy': [acc0, acc1] }\n",
    "        df=pd.DataFrame(dict1)\n",
    "        print(df.head())\n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Precision    Recall   F1Score  Accuracy\n",
      "0   0.375000  0.500000  0.428571  0.200000\n",
      "1   0.571429  0.444444  0.500000  0.266667\n"
     ]
    }
   ],
   "source": [
    "n1= Assignment12([1,1,0,0,1,0,0,1,0,1,1,1,0,1,1],[1,1,1,1,1,0,0,0,0,0,0,0,1,0,1])\n",
    "n1.getPrecision()\n",
    "n1.getRecall()\n",
    "n1.getAccuracy()\n",
    "n1.printEvaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
